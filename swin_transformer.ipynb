{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 手写swin-transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1、如何基于图片生成patch embedding?\n",
    "方法一\n",
    "- 基于pytorch unfold的API来将图片进行分块，也就是模仿卷积的思路，设置kernel_size=stride=patch_size, 得到分块后的图片\n",
    "- 得到格式为[bs, num_patch, patch_depth]的张量\n",
    "- 将张量与形状为[patch_depth, model_dim_C]的权重矩阵进行乘法操作，即可得到形状为[bs, num_patch, model_dim_C]的patch embedding\n",
    "\n",
    "方法二\n",
    "- patch_depth是等于input_channel * patch_size * patch_size\n",
    "- model_dim_C相当于二维卷积的输出通道数目\n",
    "- 将形状为[patch_depth, model_dim_C]的权重矩阵转换为[model_dim_C, input_channel, patch_size, patch_size]的卷积核\n",
    "- 调用PyTorch的conv2d API得到卷积的输出张量，形状为[bs, output_channel, height, width]\n",
    "- 转换为[bs, num_patch, model_dim_C]的格式，即为patch embedding\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 16, 100])\n",
      "torch.Size([2, 16, 100])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import math\n",
    "\n",
    "#难点1 patch embedding\n",
    "def image2emb_naive(image, patch_size, weight):\n",
    "    \"\"\"直观方法实现patch embedding\"\"\"\n",
    "    # 注意unfold的输入只针对4-D向量，所以images shape：bs*channel*h*w\n",
    "    patch_image = F.unfold(image, kernel_size=(patch_size, patch_size), stride=(patch_size, patch_size)).transpose(-1, -2)  # bc*num_patch*patch_depth(相当于一个patch的深度)\n",
    "    patch_embedding = patch_image @ weight   # bc*num_patch*model_dim_C\n",
    "    return patch_embedding\n",
    "\n",
    "def image2emb_conv2(image, kernel, stride):\n",
    "    \"\"\"基于二维卷积的patch embedding, embedding的维度就是卷积的输出通道数\"\"\"\n",
    "    out = F.conv2d(image, kernel, stride=stride)\n",
    "    bs, output_channel, height, width = out.shape\n",
    "    patch_embedding = out.reshape(bs, output_channel, height*width).transpose(-1, -2)\n",
    "    return patch_embedding\n",
    "    \n",
    "# 验证, 得到两者计算结果一致(这样构造虽然shape是对的，但是好像结果有点问题)\n",
    "patch_size = 4\n",
    "model_dim_C = 100\n",
    "images = torch.randn(2,3, 16, 16)\n",
    "weight = torch.randn(patch_size*patch_size*3, model_dim_C)\n",
    "kernel = weight.reshape(model_dim_C, 3, patch_size, patch_size)\n",
    "print(image2emb_naive(images, patch_size, weight).shape)\n",
    "print(image2emb_conv2(images, kernel, stride=patch_size).shape)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2、如何构建MHSA（多头自注意力）并计算其复杂度？\n",
    "- 基于输入x经过三个映射分别得到q，k，v\n",
    "  - 此步复杂度为$3LC^2$，其中L为序列长度，C为特征大小\n",
    "- 将q，k，v拆分成多头的形式，注意这里的多头各自计算时互不影响，所以可以与bs维度进行统一的看待\n",
    "- 计算$qk^T$，并考虑可能的掩码，即让无效的两两位置之间的能量为负无穷，掩码是在shift window MHSA中会需要，而在window MHSA中暂不需要\n",
    "  - 此步复杂度为$L^2C$\n",
    "- 计算概率值与v的乘积\n",
    "  - 此步复杂度为$LC^2$\n",
    "- 总体复杂度为$4LC^2+2L^2C$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, model_dim, num_head) -> None:\n",
    "        super().__init__()\n",
    "        self.num_head = num_head\n",
    "\n",
    "        self.proj_linear_layer = nn.Linear(model_dim, 3*model_dim)\n",
    "        self.final_linear_layer = nn.Linear(model_dim, model_dim)\n",
    "\n",
    "    def forward(self, input, additive_mask=None):\n",
    "        bs, seqlen, model_dim = input.shape\n",
    "        num_head = self.num_head\n",
    "        head_dim = model_dim // num_head\n",
    "\n",
    "        proj_out = self.proj_linear_layer(input)\n",
    "        q, k, v = proj_out.chunk(3, dim=-1)   # [bs, seqlen, model_dim] \n",
    "        # 在给定维度(轴)上将输入张量进行分块儿\n",
    "\n",
    "        # 接下来把q，k, v拆成多头的形式\n",
    "        q = q.reshape(bs, seqlen, num_head, head_dim).transpose(1, 2)  #[bs, num_head, seqlen, head_dim]\n",
    "        q = q.reshape(bs*num_head, seqlen, head_dim)\n",
    "\n",
    "        k = k.reshape(bs, seqlen, num_head, head_dim).transpose(1, 2)  #[bs, num_head, seqlen, head_dim]\n",
    "        k = k.reshape(bs*num_head, seqlen, head_dim)\n",
    "\n",
    "        v = v.reshape(bs, seqlen, num_head, head_dim).transpose(1, 2)  #[bs, num_head, seqlen, head_dim]\n",
    "        v = v.reshape(bs*num_head, seqlen, head_dim)\n",
    "\n",
    "        if additive_mask is None:\n",
    "            attn_prob = F.softmax(torch.bmm(q, k.transpose(-2, -1))/math.sqrt(head_dim), dim=-1)\n",
    "        else:\n",
    "            additive_mask = additive_mask.tile((num_head, 1, 1))\n",
    "            attn_prob = F.softmax(torch.bmm(q, k.transpose(-2, -1))/math.sqrt(head_dim)+additive_mask, dim=-1)\n",
    "\n",
    "        output = torch.bmm(attn_prob, v)    # [bs*num_head, seqlen, head_dim]\n",
    "        output = output.reshape(bs, num_head, seqlen, head_dim).transpose(1,2)\n",
    "        output = output.reshape(bs, seqlen, model_dim)\n",
    "\n",
    "        output = self.final_linear_layer(output)\n",
    "        return attn_prob, output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3、如何构建Window MHSA并计算其复杂度? (window_size固定)\n",
    "- patch组成的图片进一步划分成一个个更大的window\n",
    "  - 首先需要将三维的patch embedding转换成图片格式\n",
    "  - 使用unfold来讲patch划分成window\n",
    "- 在每个window内部计算MHSA\n",
    "  - window数目其实可以跟batchsize进行统一对待，因为window与window之间没有交互计算\n",
    "  - 关于计算复杂度\n",
    "    - 假设窗的边长为W，那么计算每个窗的总体复杂度是$4W^2C^2+2W^4C$\n",
    "    - 假设patch的总数目为L，那么窗的数目为$L/W^2$\n",
    "    - 因此，W-HMSA的总体复杂度为$4LC^2+2LW^2C$\n",
    "  - 此处不需要mask\n",
    "  - 将计算结果转换成带window的四维张量格式\n",
    "- 复杂度对比\n",
    "  - MHSA ：$4LC^2+2L^2C$\n",
    "  - W-HMSA：$4LC^2+2LW^2C$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def window_multi_head_self_attention(patch_embedding, mhsa, window_size=4, num_head=2):\n",
    "    num_patch_in_window = window_size * window_size\n",
    "    bs, num_patch, patch_depth = patch_embedding.shape\n",
    "    image_height = image_width = int(math.sqrt(num_patch))\n",
    "\n",
    "    patch_embedding = patch_embedding.transpose(-1, -2)\n",
    "    patch = patch_embedding.reshape(bs, patch_depth, image_height, image_width)\n",
    "    window = F.unfold(patch, kernel_size=(window_size, window_size), stride=(window_size, window_size)).transpose(-1,-2)\n",
    "    \n",
    "    bs, num_window, patch_depth_times_num_patch_in_window = window.shape\n",
    "    window = window.reshape(bs*num_window, patch_depth, num_patch_in_window).transpose(-1,-2)\n",
    "\n",
    "    attn_probs, output = mhsa(window)\n",
    "\n",
    "    output = output.reshape(bs, num_window, num_patch_in_window, patch_depth)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('pytorch_basic')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5d7f9c8d1ba637fbaa165b7e1b068b277a24bdbbae2fecef2adf18f32dc564ea"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
